import mlflow
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from src.utils.device import get_device, print_device_info
from src.utils.network import setup_hf_environment
import argparse
import json
import os

def load_model(model_path: str, device: str = 'auto'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œtokenizer"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    # è®¾ç½®è®¾å¤‡
    device_obj = get_device(device)
    print_device_info(device_obj)
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰tokenizeræ–‡ä»¶
    tokenizer_files = ['tokenizer.json', 'vocab.txt', 'tokenizer_config.json']
    has_tokenizer = any(os.path.exists(os.path.join(model_path, f)) for f in tokenizer_files)
    
    if not has_tokenizer:
        print("âš ï¸  æ¨¡å‹ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°tokenizeræ–‡ä»¶ï¼Œå°è¯•ä»HF-MirroråŠ è½½tokenizer...")
        try:
            # è®¾ç½®HF-Mirrorç¯å¢ƒå˜é‡
            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
            # ä»HF-MirroråŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
            # ä¿å­˜tokenizeråˆ°æ¨¡å‹ç›®å½•
            tokenizer.save_pretrained(model_path)
            print("âœ… tokenizerå·²ä¿å­˜åˆ°æ¨¡å‹ç›®å½•")
        except Exception as e:
            print(f"âŒ ä»HF-MirroråŠ è½½tokenizerå¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„tokenizer...")
            # å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜
            tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased", local_files_only=True)
            tokenizer.save_pretrained(model_path)
            print("âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜åŠ è½½tokenizeræˆåŠŸ")
    else:
        print("âœ… æ‰¾åˆ°tokenizeræ–‡ä»¶")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # åŠ è½½æ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model = model.to(device_obj)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸå¹¶ç§»åŠ¨åˆ°è®¾å¤‡")
    
    return model, tokenizer

def predict_single(model, tokenizer, prompt: str, max_length: int = 512):
    """é¢„æµ‹å•æ¡æ•°æ®"""
    input_text = prompt
    
    # tokenize
    inputs = tokenizer(input_text, truncation=True, padding=True, max_length=max_length, return_tensors="pt")
    
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # predict
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
        confidence = predictions[0][predicted_class].item()
    
    return {
        'predicted_class': predicted_class,
        'confidence': confidence,
        'predicted_label': predicted_class,
        'probabilities': predictions[0].tolist()
    }

def predict_batch(model, tokenizer, data_path: str, output_path: str, max_length: int = 512):
    """æ‰¹é‡é¢„æµ‹ LLM Router æ•°æ®"""
    results = []
    
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            
            prediction = predict_single(
                model, tokenizer, 
                data['prompt'], 
                max_length
            )
            
            result = {
                'id': data.get('id', ''),
                'prompt': data['prompt'],
                'prediction': prediction,
                'true_label': data['label']
            }
            
            results.append(result)
    
    # ä¿å­˜ç»“æœ
    with open(output_path, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    # è®¡ç®—å‡†ç¡®ç‡
    correct = sum(1 for r in results if r['prediction']['predicted_label'] == r['true_label'])
    accuracy = correct / len(results)
    
    print(f"Batch prediction complete. Accuracy: {accuracy:.4f}")
    return results

def main(args):
    # è®¾ç½®ç¯å¢ƒ
    setup_hf_environment()
    
    print("ğŸ”® å¼€å§‹é¢„æµ‹...")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.model_path, args.device)
    
    if args.single:
        # å•æ¡é¢„æµ‹
        if not args.prompt:
            raise ValueError("å•æ¡é¢„æµ‹æ¨¡å¼éœ€è¦æä¾› --prompt å‚æ•°")
        
        prediction = predict_single(
            model, tokenizer,
            args.prompt,
            args.max_length
        )
        print("Prediction:", prediction)
    else:
        # æ‰¹é‡é¢„æµ‹
        results = predict_batch(
            model, tokenizer,
            args.data_path,
            args.output_path,
            args.max_length
        )

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--single', action='store_true', help='å•æ¡é¢„æµ‹æ¨¡å¼')
    parser.add_argument('--prompt', type=str, help='æç¤ºæ–‡æœ¬ï¼ˆå•æ¡é¢„æµ‹æ¨¡å¼ï¼‰')
    parser.add_argument('--data_path', type=str, default='data/llm_router_dataset-synth/test.jsonl', help='æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ‰¹é‡é¢„æµ‹æ¨¡å¼ï¼‰')
    parser.add_argument('--output_path', type=str, default='predictions.jsonl', help='é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„')
    parser.add_argument('--max_length', type=int, default=512, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--device', type=str, default='auto', 
                       choices=['auto', 'cpu', 'cuda', 'mps'],
                       help='è®¾å¤‡é€‰æ‹©: auto(è‡ªåŠ¨), cpu, cuda, mps')
    args = parser.parse_args()
    main(args)

