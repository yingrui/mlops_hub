# MLOps实践指南

## 关注机器学习项目，CD4ML加速模型迭代

## 摘要

近年来，MLOps在行业内获得了广泛关注与应用。以MLOps为核心工具的初创公司不断涌现，各大云平台厂商也相继推出了较为完整的MLOps工具链。在企业内部，随着AI应用场景的拓展和深化，MLOps的实施已获得组织层面的关注。部分领先企业已将机器学习模型视为核心资产，着手实施全生命周期管理。当前MLOps落地的核心挑战，已从价值与工具的缺失，转变为缺乏适配组织特定场景的最佳实践指导，这直接导致了工程化效率的低下。

尽管各种机器学习任务的模型训练方法与工具已相对成熟，但其工程化实践仍显不足。具体表现为：

1. **工作方式碎片化**： AI平台的出现并未显著减少"小作坊"式的开发方式。
2. **协作壁垒高筑**： 跨角色或团队间因缺乏共同的技术与知识基础，协作与沟通依旧困难重重。
3. **资产管理薄弱**： 对AI资产的管理与运营普遍不足，导致难以有效沉淀和复用。
4. **部署效率低下**： 受限于工程能力或经验，模型部署困难、上线周期长的问题依然突出。
5. **监控与迭代缺失**： 模型的持续监控、迭代升级方法与运营体系，基本只存在于少数成熟的AI产品团队中。

上述挑战的深层根源，在于机器学习项目（后简称ML项目）开发与交付过程中普遍缺乏统一的工程化标准规范。这导致了实践的不一致性、协作的低效和浪费。

在Web开发早期，项目同样面临代码混乱、协作困难、部署复杂等问题。成熟Web开发框架（如Spring, fastapi等）的出现，通过提供预设的项目结构、约定的配置、内置的最佳实践，并结合持续集成自动化工具链，彻底改变了局面。其核心在于将项目内容（代码、配置、资源）和组织方式（开发、测试和运维等角色的协作流程） 纳入一个规范和标准的体系。结果是：开发效率显著提升、代码质量更可控、团队协作更顺畅、项目可维护性增强、新成员上手更快。

ML项目亟需类似的范式转变。构建并采用适配组织的工程化框架与标准规范，是解决当前痛点、提升交付效率和质量的关键。其核心要素包括：

1. **项目结构标准化**： 定义清晰的数据、代码、模型、配置目录结构，确保项目可理解、可复现。
2. **代码规范化**： 明确定义从数据准备、实验、训练、验证、部署到监控的端到端流水线任务。
3. **接口契约化**： 规定模型服务接口、特征和数据格式等，降低模块间耦合度，提升互操作性。
4. **工具链集成**： 围绕标准和流程，集成CI/CD版本控制、数据集管理、任务调度与编排（K8S、Kubeflow、Airflow等）、模型仓库（MLflow）、监控（系统监控、模型监控）等工具。同时支持资产管理制度的落地：明确模型、特征、数据集、实验记录等核心资产的注册、存储和版本。

CD4ML[1]（Continuous Delivery for Machine Learning）正是这种工程化框架理念在MLOps领域的核心体现。 它借鉴并扩展了软件工程中持续交付的思想，提倡一切皆代码（EaC: Everything as Code）和版本化管理的实践，构建标准化的ML项目结构并持续完善代码，并定义自动化的、可重复的机器学习流水线。通过实施以CD4ML为核心的工程化框架和标准规范，组织能够显著提升效率、保障交付质量、打破协作壁垒、促进资产沉淀和复用、增强可维护性与合规性。

因此，推动MLOps落地的核心，不仅是引入工具，也需要建立一套类开发框架的工程化标准和规范体系。CD4ML为实现这一目标提供了方法论和最佳实践路径，将从根本上改进ML项目的交付效率、质量和可持续性。 本指南后续章节将详细阐述如何构建这样一个规范化的、高效的ML项目和自动化机器学习流水线。

## MLOps实施原则及落地
理解MLOps实施原则是成功落地的关键。在2021年谷歌的MLOps实践指南[2]中提出机器学习工程在实际应用中存在特有的复杂性，这些复杂性包括：高质量训练数据、在线模型监测、持续更新数据集并自动化开展实验、避免训练-生产环境的偏差、以及模型公平性和对抗攻击的应对等。信通院2023年发布的实践指南[3]中总结出MLOps的实施原则包括：自动化、持续性、版本化、可监控、可测试、可追溯、可复现、可协作等。

单纯依靠工具链是无法应对上述复杂性并满足实施原则的要求。工具提供了必要的能力支撑，但原则的真正落地也取决于ML项目本身的实现。下表展示了实施原则与落地载体之间关系的分析：

| 实施原则 | 数据集管理工具 | 模型训练平台 | 模型仓库 | 模型推理服务 | ML项目 |
|---------|---------------|-------------|----------|-------------|--------|
| 自动化 | -- | 任务调度管理 | -- | 支持模型部署 | 流水线开发 |
| 版本化 | 数据集版本管理 | -- | 模型版本管理 | 推理服务版本 | 结合各工具实现版本管理 |
| 可复现 | 同上 | -- | 存储实验记录 | -- | 记录实验参数和结果 |
| 可协作 | 数据共享 | 系统监控 | -- | 集成测试支持 | 规范的目录结构与代码 |
| 可测试 | 跨环境访问 | 跨环境部署 | 跨环境部署 | 跨环境部署 | 各种测试用例开发 |
| 可监控 | -- | 训练过程日志 | -- | 推理日志记录 | 监控流水线&自定义日志 |
| 可追溯 | 审计日志 | 审计日志 | 实验与模型管理 | 审计日志 | 身份认证和密钥管理 |
| 持续性 | -- | -- | -- | -- | 开发数据闭环任务和工具 |

**表1: MLOps实施原则及关键落地载体**

根据上表可以看出，数据集管理、模型训练、模型仓库、模型推理等工具仅对各实施原则提供了支持。因此，MLOps实施原则的真正有效落地，高度依赖于ML项目中精心设计和实现的代码，特别是ML项目中的离线任务和在线服务。

### 离线（流水线）任务：
- 数据获取、清洗、挖掘流水线
- 数据集分析与预标注流水线
- 数据集特征工程和处理流水线
- 模型训练、验证、评估流水线
- 离线模型推理流水线
- 模型监控流水线

### 在线（推理）服务：
- 多模型组合（复合AI）推理服务
- 单节点多模型推理服务
- 影子部署/AB测试路由

这些ML项目中的流水线和推理服务还必须：

1. 深度集成各类DevOps、DataOps和MLOps工具链（利用其API进行数据存取、模型管理、任务触发、监控指标上报等）。
2. 严格遵守团队制定的工程化规范（项目结构、代码风格、接口契约、测试覆盖、版本控制、日志规范等），确保其本身满足自动化、可测试、可监控、可追溯、可复现、可协作等要求。

## ML项目

和其他软件项目相同的是，ML项目也是一个代码仓库。它包含高质量的数据集挖掘、处理与机器学习模型训练、评估、检测等代码，以及与MLOps工具集成的代码。为了便于理解ML项目的重要性，本指南将通过不断的完善一个简单的ML项目，来体现ML项目的重要性，以及与工程化结合。

### 模型训练

#### 训练算法

随着机器学习算法框架的不断成熟，很多任务仅需要通过一段脚本，再给定数据即可开始训练模型。

```python
# Transformer based text classification model training script
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=args.output_dir,
    …
)
        
trainer = Trainer(
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    …
)

trainer.train()
…
```

以文本分类为例，除了基于Transformer的模型，还是基于TF-IDF或其它方法，选择各种模型的训练算法都非常简便。

#### 可复现与可追溯原则落地

但完成模型训练仅是工程化的一个环节，站在可信的角度，可复现与可追溯对于团队的成员来说十分重要。举一个工作交接的例子，也许就能引发很多人的共鸣："同事负责的模型交接给你，三个月后因为Python和PyTorch版本需要升级，花费了大量时间从原始数据开始处理，并且因为参数没有记录，多次训练的模型效果还无法达到过去的水平。"

如果不借助数据集管理工具、代码管理工具（git）、模型管理工具（MLflow）等，将面临数据复杂性、代码和环境不确定、参数配置记录和实验对比的缺失，以及协作和共享困难等诸多不便。工程师们需要建立起实验的概念，追踪每次实验运行的数据和结果。

```python
mlflow.set_experiment(args.experiment_name) # 指定实验名称

with mlflow.start_run() as run:      # 开始实验
    mlflow.log_params(vars(args)) # 记录实验参数
    mlflow.log_input(train_dataset) # 记录训练数据集

    trainer.train()
    metrics = trainer.evaluate()

    mlflow.log_metrics(metrics) # 记录模型评估结果

    mlflow.log_artifact(model_save_path) # 记录训练结果文件
    …
```

通过在之前的代码中添加与MLflow集成的代码，得以追踪每次实验的相关信息。但如果团队对工程能力不够关注，很多机器学习工程师并不会主动与MLflow进行集成。而且MLflow本身提供的API也不一定完全满足项目的要求，例如log_input方法，可能就需要进行一些定制化的改造。因为log_input方法本身是将数据集的元信息记录下来，而MLflow所支持的数据集类型也仅有常见的类型。以下是MLflow中PandasDataset的例子，如果需要借助MLflow记录下自定义数据集的信息，可通过继承Dataset类来实现。

```python
class PandasDataset(Dataset, PyFuncConvertibleDatasetMixin): # 指定实验名称
    …
    def to_dict(self) -> dict[str, str]:
        """Create config dictionary for the dataset.

        Returns a string dictionary containing the following fields: name, digest, source, source
        type, schema, and profile.
        """
        schema = json.dumps({"mlflow_colspec": self.schema.to_dict()}) if self.schema else None
        config = super().to_dict()
        config.update(
            {
                "schema": schema,
                "profile": json.dumps(self.profile),
            }
        )
        return config
```

这并不是专门介绍MLflow的用法，而是想进一步强调：从工程化的角度看ML项目，也需要关注集成的代码开发，这样才能更好的发挥各种MLOps工具的作用。同时，还需关注集成代码和训练代码的解耦，让算法演进与工程规范都能独立的发展。

### 自动化任务

#### 训练任务

开发完模型训练脚本，下一步应该开始训练了。但是模型训练需要的资源可能个人工作站无法满足，于是需要将训练任务运行在其他环境中。

而很多情况下，如果团队能够申请到单独的GPU资源，可以就像在本地环境中运行一样，远程登录或启动Jupyter服务运行训练任务。但这样的方式同时也意味着GPU资源被独占，大概率GPU利用率也会非常低。

当然在假设无限资源的时候，独享资源可能是大多数团队最喜欢的方式。但为了降低算力成本，提升资源利用率，自动化运行训练任务就显得重要起来。

支持机器学习开发的云或PaaS平台厂商都提供了自动化任务管理的服务，支持通过自定义容器+命令行启动训练任务（或其他各种类型的任务）。通过开发一些本地命令行工具，既可以方便开发人员的操作，也可以方便与各种任务管理系统集成。

#### 本地命令行工具

支持本地命令行工具开发的工具有众多选择，例如团队选择fastapi开发推理服务的话，可能就会使用Typer来开发命令行工具。通常的命令行工具支持：

- **数据集或文件管理工具**：查询、下载、上传数据集或文件，也可以考虑支持数据集分析与挖掘
- **任务管理工具**：启动模型训练等各类任务，查询任务状态
- **模型管理工具**：查询、下载模型并支持模型推理，以及注册一些公共的基础模型

本地命令行工具的开发，既能有助于在本地快速完成一系列操作，也能促进协作。举例来说，业务团队提供的数据格式往往与训练数据集不同，数据科学家或算法工程师可能开发了很多数据处理脚本，但这些脚本往往是随意的添加到ML项目中，经常在交接工作时出现脚本或参数遗忘的情况。而如果添加成命令行工具，按照规范要求的文档和命令行帮助信息，这些都能让相对正式的命令行工具可以更容易被应用和维护。

#### 跨环境自动化

开发命令行工具非常有利于跨环境运行各类任务。而且通过环境变量设置和不同的配置文件，通常情况下已经可以在不同环境中按配置运行各类任务。尤其值得注意的是，数据科学家和算法工程师往往会忽视的是密钥密码等敏感信息的配置，而这些也需要按照工程规范要求开发。

而当团队负责了多个ML项目，这样就会存在多次与MLOps工具集成的重复工作，所以与MLOps工具集成的代码可以封装成SDK，这样可以提高工程质量以及开发效率。

### 推理服务

#### 批量模型推理

当模型训练完成之后，那接下来就是将模型应用到生产环境中了。结合MLflow等模型仓库工具，在训练完成之后在模型仓库中可以得到各个版本的模型文件，以及各次实验的模型评估指标。所以最常见的方式就是在后台根据实验结果选择模型，对输入数据进行处理后加载模型进行推理。

在ML项目中将模型推理并封装成命令行工具，输入输出可以用文件的形式，这样可以方便的封装成离线推理任务，还可以与各种编排工具集成，实现批量模型推理。

除了典型的任务编排的方式之外，可能还需考虑与大数据平台集成。或者被集成到其他分布式计算系统中，例如与Temporal等工作流执行系统集成。

通过任务的方式实现批量模型推理的好处是，模型并不需要一直占用GPU等计算资源，而是在任务执行的过程中加载并完成推理，任务结束后计算资源就可以释放了。但这种方式的问题是模型加载往往需要较长时间，如果需要在很短时间内完成推理的话，就需要考虑部署在线推理服务了。

#### 在线推理服务

将模型部署成为一个服务，常见的方式有两种：
1. 将模型部署到一个推理服务中
2. 启动一个加载了某个模型的推理服务

#### 复合模型推理 (多个模型(dummy))

#### 推理服务网关

### 模型监控与数据闭环

### 模型训练与评估

### 实验记录与评估

### 模型监控(shift and etc.)

### ML应用

- 展示类应用(Gradio/ HF Demo?)
- 作业及标注类应用(推理服务的日志 - 置信度)

## 资产管理与协作推广

### 角色与协作

### 高代码与低代码

## MLOps成熟度评估


## 参考文献

[1] CD4ML: Continuous Delivery for Machine Learning, thoughtworks, 2019  
[2] Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning, Google, 2021  
[3] 人工智能研发运营体系（MLOps）实践指南，信通院，2023  
[4] The Big Book of MLOps 2nd edition, databricks, 2023
